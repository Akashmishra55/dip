Here's the code for each practical, provided separately and clearly numbered:


---

Practical 1: Web Scraping Court Case Summaries

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://www.indiankanoon.org/search/?formInput=section%20378'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

summaries = []
for div in soup.find_all('div', class_='result_title'):
    link = 'https://www.indiankanoon.org' + div.a['href']
    title = div.text.strip()
    summaries.append({'Title': title, 'Link': link})

df = pd.DataFrame(summaries)
df.to_csv('court_case_summaries.csv', index=False)


---

Practical 2: EDA on Legal Case Dataset

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('legal_cases.csv')  # Assume file is prepared

print(df.describe())
print(df.mode())

sns.countplot(data=df, x='Case_Outcome')
plt.title('Distribution of Case Outcomes')
plt.show()

sns.countplot(data=df, x='Case_Type')
plt.title('Frequency of Case Types')
plt.xticks(rotation=90)
plt.show()


---

Practical 3: Text Preprocessing

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

text = "The petitioner was charged under section 420. The case was dismissed."

tokens = word_tokenize(text)
sentences = sent_tokenize(text)

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

processed = [lemmatizer.lemmatize(stemmer.stem(w.lower())) for w in tokens if w.isalpha() and w.lower() not in stop_words]
print(processed)


---

Practical 4: Sentiment Analysis

from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

text = "The judge ruled in favor of the defendant, citing insufficient evidence."

# TextBlob
blob = TextBlob(text)
print("TextBlob Sentiment:", blob.sentiment)

# VADER
analyzer = SentimentIntensityAnalyzer()
vader_score = analyzer.polarity_scores(text)
print("VADER Sentiment:", vader_score)


---

Practical 5: ML Model for Case Outcome

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

df = pd.read_csv('legal_cases.csv')
le = LabelEncoder()
df['Outcome'] = le.fit_transform(df['Outcome'])
X = df[['Feature1', 'Feature2']]  # Replace with actual features
y = df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LogisticRegression()
model.fit(X_train, y_train)
pred = model.predict(X_test)

print(classification_report(y_test, pred))


---

Practical 6: Clustering Legal Documents

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

docs = ["Document text 1...", "Document text 2...", "Document text 3..."]

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(docs)

kmeans = KMeans(n_clusters=3)
kmeans.fit(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_)
plt.title('KMeans Clustering of Legal Documents')
plt.show()


---

Practical 7: Topic Modeling with LDA

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

documents = ["The court held...", "The plaintiff argued...", "The defendant..."]

vectorizer = CountVectorizer(stop_words='english')
doc_term_matrix = vectorizer.fit_transform(documents)

lda = LatentDirichletAllocation(n_components=2)
lda.fit(doc_term_matrix)

for idx, topic in enumerate(lda.components_):
    print(f"Topic {idx}:", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])


---

Practical 8: Named Entity Recognition (NER)

import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
text = "On 5th June 2021, the Supreme Court of India ruled in favor of Infosys Ltd."

doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)

displacy.render(doc, style="ent", jupyter=False)


---

Practical 9: Extract Key Clauses from Contracts

import re

with open('contract.txt', 'r') as file:
    contract_text = file.read()

termination = re.findall(r'(termination.*?\.)', contract_text, flags=re.IGNORECASE | re.DOTALL)
confidentiality = re.findall(r'(confidentiality.*?\.)', contract_text, flags=re.IGNORECASE | re.DOTALL)

print("Termination Clauses:\n", termination)
print("Confidentiality Clauses:\n", confidentiality)


---

Practical 10: Legal QA System with BERT

from transformers import pipeline

qa_pipeline = pipeline("question-answering", model="deepset/bert-base-cased-squad2")

context = "The Supreme Court ruled in favor of the plaintiff, awarding damages."
question = "Who did the Supreme Court rule in favor of?"

result = qa_pipeline(question=question, context=context)
print("Answer:", result['answer'])


---

Let me know if you need the datasets, sample outputs, or explanation notebooks!

